#!/usr/bin/env python3
"""
Complete Evaluation Script for Enhanced ABSA with 2024-2025 Breakthrough Features
Supports comprehensive evaluation including contrastive learning and few-shot metrics
"""

import os
import sys
import argparse
import json
import time
from datetime import datetime
import traceback
import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.utils.config import LLMABSAConfig
from src.models.absa import LLMABSA
from src.data.dataset import ABSADataset, MultiDomainABSADataset
from src.data.preprocessor import ABSAPreprocessor
from src.training.metrics import (
    calculate_comprehensive_metrics, 
    calculate_triplet_recovery_score,
    calculate_enhanced_metrics,
    analyze_learning_curves
)
from src.utils.logger import setup_logger
from src.models.few_shot_learner import FewShotABSAEvaluator

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Enhanced ABSA Evaluation')
    
    # Model and data arguments
    parser.add_argument('--model', type=str, required=True,
                       help='Path to trained model checkpoint')
    parser.add_argument('--dataset', type=str, default='rest15',
                       choices=['laptop14', 'rest14', 'rest15', 'rest16', 'all'],
                       help='Dataset to evaluate on')
    parser.add_argument('--data_dir', type=str, default='Dataset/aste',
                       help='Data directory')
    parser.add_argument('--split', type=str, default='test',
                       choices=['dev', 'test', 'all'],
                       help='Data split to evaluate')
    
    # Evaluation options
    parser.add_argument('--batch_size', type=int, default=16,
                       help='Evaluation batch size')
    parser.add_argument('--max_length', type=int, default=128,
                       help='Maximum sequence length')
    
    # 2024-2025 Breakthrough Evaluation Features
    parser.add_argument('--eval_contrastive', action='store_true', default=True,
                       help='Evaluate contrastive learning components')
    parser.add_argument('--eval_few_shot', action='store_true', default=True,
                       help='Evaluate few-shot learning performance')
    parser.add_argument('--eval_cross_domain', action='store_true', default=True,
                       help='Evaluate cross-domain transfer')
    parser.add_argument('--eval_implicit', action='store_true', default=True,
                       help='Evaluate implicit aspect/opinion detection')
    
    # Advanced evaluation options
    parser.add_argument('--save_predictions', action='store_true',
                       help='Save detailed predictions to file')
    parser.add_argument('--error_analysis', action='store_true',
                       help='Perform detailed error analysis')
    parser.add_argument('--visualize_attention', action='store_true',
                       help='Visualize attention patterns')
    
    # Few-shot evaluation parameters
    parser.add_argument('--few_shot_k', type=int, default=5,
                       help='Number of shots for few-shot evaluation')
    parser.add_argument('--few_shot_episodes', type=int, default=100,
                       help='Number of episodes for few-shot evaluation')
    
    # Output options
    parser.add_argument('--output_dir', type=str, default='results',
                       help='Output directory for results')
    parser.add_argument('--verbose', action='store_true',
                       help='Verbose output')
    
    return parser.parse_args()

def load_model_and_config(model_path, device):
    """Load trained model and configuration"""
    print(f"üìÇ Loading model from: {model_path}")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model checkpoint not found: {model_path}")
    
    # Load checkpoint
    checkpoint = torch.load(model_path, map_location=device)
    
    # Create config (this should ideally be saved with the model)
    config = LLMABSAConfig()
    if 'config' in checkpoint:
        # If config was saved with checkpoint
        saved_config = checkpoint['config']
        for key, value in saved_config.items():
            if hasattr(config, key):
                setattr(config, key, value)
    
    # Initialize model
    model = LLMABSA(config)
    
    # Load state dict
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model = model.to(device)
    model.eval()
    
    # Get training info if available
    training_info = {
        'epoch': checkpoint.get('epoch', 'unknown'),
        'metrics': checkpoint.get('metrics', {}),
        'timestamp': checkpoint.get('timestamp', 'unknown')
    }
    
    print(f"‚úÖ Model loaded successfully")
    print(f"üìä Training epoch: {training_info['epoch']}")
    if training_info['metrics']:
        best_f1 = training_info['metrics'].get('val_f1', 'unknown')
        print(f"üèÜ Best validation F1: {best_f1}")
    
    return model, config, training_info

def evaluate_standard_metrics(model, dataloader, device, config, save_predictions=False):
    """
    Evaluate standard ABSA metrics
    
    Returns comprehensive evaluation results
    """
    print("üìä Evaluating standard ABSA metrics...")
    
    model.eval()
    all_predictions = []
    all_targets = []
    all_samples = []
    
    total_loss = 0
    num_batches = 0
    
    # For detailed analysis
    prediction_details = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="Standard Evaluation")):
            try:
                # Move batch to device
                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                        for k, v in batch.items()}
                
                # Forward pass
                outputs = model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    texts=batch.get('texts', None),
                    task_type='triplet_extraction'
                )
                
                if outputs is None:
                    print(f"‚ö† Warning: Model returned None for batch {batch_idx}")
                    continue
                
                # Extract predictions
                predictions = model.extract_triplets(
                    outputs=outputs,
                    input_ids=batch['input_ids'],
                    texts=batch.get('texts', batch.get('original_texts', []))
                )
                
                targets = batch.get('triplets', [])
                
                all_predictions.extend(predictions)
                all_targets.extend(targets)
                
                # Save sample details if requested
                if save_predictions:
                    for i in range(len(predictions)):
                        sample_detail = {
                            'text': batch['texts'][i] if i < len(batch['texts']) else '',
                            'predicted_triplets': predictions[i],
                            'true_triplets': targets[i] if i < len(targets) else [],
                            'tokens': batch['tokens'][i] if 'tokens' in batch and i < len(batch['tokens']) else []
                        }
                        prediction_details.append(sample_detail)
                
                num_batches += 1
                
            except Exception as e:
                print(f"‚ùå Error in batch {batch_idx}: {e}")
                if config.debug_mode:
                    traceback.print_exc()
                continue
    
    # Calculate comprehensive metrics
    try:
        metrics = calculate_comprehensive_metrics(all_predictions, all_targets, config)
        
        # Add enhanced metrics for 2024-2025 evaluation
        enhanced_metrics = calculate_enhanced_metrics(all_predictions, all_targets, config)
        metrics.update(enhanced_metrics)
        
        # Calculate Triplet Recovery Score (TRS) - 2024-2025 innovation
        trs_score = calculate_triplet_recovery_score(all_predictions, all_targets)
        metrics['triplet_recovery_score'] = trs_score
        
        print(f"üìà Standard Evaluation Results:")
        print(f"   F1 Score: {metrics.get('f1', 0):.4f}")
        print(f"   Precision: {metrics.get('precision', 0):.4f}")
        print(f"   Recall: {metrics.get('recall', 0):.4f}")
        print(f"   Aspect F1: {metrics.get('aspect_f1', 0):.4f}")
        print(f"   Opinion F1: {metrics.get('opinion_f1', 0):.4f}")
        print(f"   Sentiment Accuracy: {metrics.get('sentiment_accuracy', 0):.4f}")
        print(f"   Triplet Recovery Score: {metrics.get('triplet_recovery_score', 0):.4f}")
        
    except Exception as e:
        print(f"‚ùå Error calculating metrics: {e}")
        metrics = {
            'f1': 0.0, 'precision': 0.0, 'recall': 0.0,
            'error': str(e)
        }
    
    evaluation_results = {
        'metrics': metrics,
        'num_samples': len(all_predictions),
        'num_batches': num_batches
    }
    
    if save_predictions:
        evaluation_results['prediction_details'] = prediction_details
    
    return evaluation_results

def evaluate_few_shot_performance(model, datasets, config, args):
    """
    Evaluate few-shot learning performance
    
    2024-2025 breakthrough: Comprehensive few-shot evaluation
    """
    print("üéØ Evaluating few-shot learning performance...")
    
    if not hasattr(model, 'few_shot_learner') or model.few_shot_learner is None:
        print("‚ö† Model doesn't have few-shot learning components, skipping...")
        return {'status': 'not_available'}
    
    # Initialize few-shot evaluator
    evaluator = FewShotABSAEvaluator(config)
    
    # Prepare datasets for few-shot evaluation
    domain_datasets = {}
    if isinstance(datasets, dict):
        domain_datasets = datasets
    else:
        # Single domain dataset
        domain_datasets[args.dataset] = {'test': datasets}
    
    # Evaluate few-shot performance
    few_shot_results = evaluator.evaluate_few_shot_performance(
        model, domain_datasets, list(domain_datasets.keys())
    )
    
    # Print results
    print(f"üìä Few-Shot Learning Results ({args.few_shot_k}-shot):")
    for domain, results in few_shot_results.items():
        print(f"   {domain}:")
        print(f"     Accuracy: {results.get('accuracy_mean', 0):.4f} ¬± {results.get('accuracy_std', 0):.4f}")
        print(f"     F1 Score: {results.get('f1_mean', 0):.4f} ¬± {results.get('f1_std', 0):.4f}")
    
    return few_shot_results

def evaluate_cross_domain_transfer(model, multi_domain_dataset, config, args):
    """
    Evaluate cross-domain transfer learning
    
    2024-2025 breakthrough: Cross-domain robustness evaluation
    """
    print("üåê Evaluating cross-domain transfer performance...")
    
    if not isinstance(multi_domain_dataset, MultiDomainABSADataset):
        print("‚ö† Multi-domain dataset required for cross-domain evaluation, skipping...")
        return {'status': 'not_available'}
    
    domains = multi_domain_dataset.domains
    if len(domains) < 2:
        print("‚ö† At least 2 domains required for cross-domain evaluation, skipping...")
        return {'status': 'insufficient_domains'}
    
    # Initialize few-shot evaluator for cross-domain evaluation
    evaluator = FewShotABSAEvaluator(config)
    
    # Prepare domain datasets
    domain_datasets = {}
    for domain in domains:
        domain_datasets[domain] = multi_domain_dataset.datasets[domain]
    
    # Evaluate cross-domain transfer
    transfer_results = evaluator.evaluate_cross_domain_transfer(
        model, domains[:2], domains[2:] if len(domains) > 2 else domains[:1], 
        domain_datasets
    )
    
    # Print results
    print(f"üìä Cross-Domain Transfer Results:")
    for transfer_pair, results in transfer_results.items():
        print(f"   {transfer_pair}:")
        print(f"     Accuracy: {results.get('accuracy', 0):.4f}")
        print(f"     F1 Score: {results.get('macro_f1', 0):.4f}")
    
    return transfer_results

def evaluate_implicit_detection(model, dataloader, device, config):
    """
    Evaluate implicit aspect/opinion detection
    
    2024-2025 breakthrough: Specialized evaluation for implicit sentiment
    """
    print("üîç Evaluating implicit aspect/opinion detection...")
    
    model.eval()
    implicit_predictions = []
    implicit_targets = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="Implicit Detection Evaluation")):
            try:
                # Move batch to device
                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                        for k, v in batch.items()}
                
                # Forward pass with implicit detection task
                outputs = model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    texts=batch.get('texts', None),
                    task_type='implicit_detection'
                )
                
                if outputs is None:
                    continue
                
                # Extract implicit predictions
                predictions = model.extract_triplets(
                    outputs=outputs,
                    input_ids=batch['input_ids'],
                    texts=batch.get('texts', batch.get('original_texts', [])),
                    focus_implicit=True
                )
                
                # Filter for implicit targets
                targets = batch.get('triplets', [])
                implicit_targets_batch = []
                
                for sample_targets in targets:
                    implicit_sample_targets = []
                    for triplet in sample_targets:
                        # Check if triplet contains implicit aspects or opinions
                        aspect_indices = triplet.get('aspect_indices', [])
                        opinion_indices = triplet.get('opinion_indices', [])
                        
                        # Consider as implicit if indices are empty or point to null/implicit markers
                        if not aspect_indices or not opinion_indices:
                            implicit_sample_targets.append(triplet)
                    
                    implicit_targets_batch.append(implicit_sample_targets)
                
                implicit_predictions.extend(predictions)
                implicit_targets.extend(implicit_targets_batch)
                
            except Exception as e:
                print(f"‚ùå Error in implicit detection batch {batch_idx}: {e}")
                continue
    
    # Calculate implicit-specific metrics
    try:
        implicit_metrics = calculate_comprehensive_metrics(implicit_predictions, implicit_targets, config)
        
        print(f"üìä Implicit Detection Results:")
        print(f"   Implicit F1: {implicit_metrics.get('f1', 0):.4f}")
        print(f"   Implicit Precision: {implicit_metrics.get('precision', 0):.4f}")
        print(f"   Implicit Recall: {implicit_metrics.get('recall', 0):.4f}")
        
    except Exception as e:
        print(f"‚ùå Error calculating implicit metrics: {e}")
        implicit_metrics = {'error': str(e)}
    
    return {
        'implicit_metrics': implicit_metrics,
        'num_implicit_samples': len(implicit_predictions)
    }

def perform_error_analysis(evaluation_results, output_dir):
    """
    Perform detailed error analysis
    
    2024-2025 enhancement: Advanced error categorization and analysis
    """
    print("üîç Performing detailed error analysis...")
    
    if 'prediction_details' not in evaluation_results:
        print("‚ö† Prediction details not available for error analysis")
        return {}
    
    prediction_details = evaluation_results['prediction_details']
    
    # Error categories
    error_categories = {
        'aspect_extraction_errors': 0,
        'opinion_extraction_errors': 0,
        'sentiment_classification_errors': 0,
        'triplet_pairing_errors': 0,
        'implicit_detection_errors': 0,
        'false_positives': 0,
        'false_negatives': 0,
        'partial_matches': 0
    }
    
    error_examples = {category: [] for category in error_categories}
    
    for sample in prediction_details:
        text = sample['text']
        predicted = sample['predicted_triplets']
        true_triplets = sample['true_triplets']
        
        # Analyze errors for this sample
        sample_errors = analyze_sample_errors(predicted, true_triplets, text)
        
        # Update error counts
        for error_type, count in sample_errors['counts'].items():
            if error_type in error_categories:
                error_categories[error_type] += count
        
        # Collect error examples
        for error_type, examples in sample_errors['examples'].items():
            if error_type in error_examples:
                error_examples[error_type].extend(examples[:3])  # Limit examples
    
    # Calculate error rates
    total_samples = len(prediction_details)
    error_rates = {k: v / total_samples for k, v in error_categories.items()}
    
    # Create error analysis report
    error_analysis = {
        'error_categories': error_categories,
        'error_rates': error_rates,
        'error_examples': error_examples,
        'total_samples': total_samples
    }
    
    # Save error analysis
    error_analysis_path = os.path.join(output_dir, 'error_analysis.json')
    with open(error_analysis_path, 'w') as f:
        json.dump(error_analysis, f, indent=2)
    
    print(f"üìä Error Analysis Summary:")
    print(f"   Aspect extraction errors: {error_rates['aspect_extraction_errors']:.2%}")
    print(f"   Opinion extraction errors: {error_rates['opinion_extraction_errors']:.2%}")
    print(f"   Sentiment classification errors: {error_rates['sentiment_classification_errors']:.2%}")
    print(f"   Triplet pairing errors: {error_rates['triplet_pairing_errors']:.2%}")
    print(f"   False positives: {error_rates['false_positives']:.2%}")
    print(f"   False negatives: {error_rates['false_negatives']:.2%}")
    
    print(f"‚úÖ Error analysis saved to: {error_analysis_path}")
    
    return error_analysis

def analyze_sample_errors(predicted_triplets, true_triplets, text):
    """Analyze errors in a single sample"""
    errors = {
        'counts': {
            'aspect_extraction_errors': 0,
            'opinion_extraction_errors': 0,
            'sentiment_classification_errors': 0,
            'triplet_pairing_errors': 0,
            'false_positives': 0,
            'false_negatives': 0
        },
        'examples': {
            'aspect_extraction_errors': [],
            'opinion_extraction_errors': [],
            'sentiment_classification_errors': [],
            'triplet_pairing_errors': [],
            'false_positives': [],
            'false_negatives': []
        }
    }
    
    # Convert to comparable format
    pred_set = set()
    true_set = set()
    
    for triplet in predicted_triplets:
        if isinstance(triplet, dict):
            aspect = triplet.get('aspect', '')
            opinion = triplet.get('opinion', '')
            sentiment = triplet.get('sentiment', '')
            pred_set.add((aspect, opinion, sentiment))
    
    for triplet in true_triplets:
        if isinstance(triplet, dict):
            aspect = triplet.get('aspect', '')
            opinion = triplet.get('opinion', '')
            sentiment = triplet.get('sentiment', '')
            true_set.add((aspect, opinion, sentiment))
    
    # Count different types of errors
    errors['counts']['false_positives'] = len(pred_set - true_set)
    errors['counts']['false_negatives'] = len(true_set - pred_set)
    
    # Add examples
    for fp in list(pred_set - true_set)[:3]:
        errors['examples']['false_positives'].append({
            'text': text,
            'predicted': fp,
            'note': 'Not in ground truth'
        })
    
    for fn in list(true_set - pred_set)[:3]:
        errors['examples']['false_negatives'].append({
            'text': text,
            'missed': fn,
            'note': 'Missing from predictions'
        })
    
    return errors

def create_evaluation_visualizations(results, output_dir):
    """Create visualizations for evaluation results"""
    print("üìä Creating evaluation visualizations...")
    
    # Set up matplotlib
    plt.style.use('seaborn-v0_8')
    
    # 1. Metrics Comparison Chart
    metrics = results.get('standard_evaluation', {}).get('metrics', {})
    if metrics:
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        
        metric_names = ['F1', 'Precision', 'Recall', 'Aspect F1', 'Opinion F1']
        metric_values = [
            metrics.get('f1', 0),
            metrics.get('precision', 0),
            metrics.get('recall', 0),
            metrics.get('aspect_f1', 0),
            metrics.get('opinion_f1', 0)
        ]
        
        bars = ax.bar(metric_names, metric_values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])
        ax.set_ylabel('Score')
        ax.set_title('ABSA Evaluation Metrics')
        ax.set_ylim(0, 1)
        
        # Add value labels on bars
        for bar, value in zip(bars, metric_values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{value:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'metrics_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    # 2. Error Analysis Chart
    if 'error_analysis' in results:
        error_data = results['error_analysis']['error_rates']
        
        fig, ax = plt.subplots(1, 1, figsize=(12, 8))
        
        error_types = list(error_data.keys())
        error_rates = list(error_data.values())
        
        bars = ax.barh(error_types, error_rates, color='lightcoral')
        ax.set_xlabel('Error Rate')
        ax.set_title('Error Analysis by Category')
        
        # Add value labels
        for bar, rate in zip(bars, error_rates):
            width = bar.get_width()
            ax.text(width + 0.001, bar.get_y() + bar.get_height()/2.,
                   f'{rate:.2%}', ha='left', va='center')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'error_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    # 3. Few-shot Performance Chart
    if 'few_shot_evaluation' in results and results['few_shot_evaluation'].get('status') != 'not_available':
        few_shot_data = results['few_shot_evaluation']
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        domains = list(few_shot_data.keys())
        accuracy_means = [few_shot_data[d].get('accuracy_mean', 0) for d in domains]
        accuracy_stds = [few_shot_data[d].get('accuracy_std', 0) for d in domains]
        f1_means = [few_shot_data[d].get('f1_mean', 0) for d in domains]
        f1_stds = [few_shot_data[d].get('f1_std', 0) for d in domains]
        
        # Accuracy chart
        ax1.bar(domains, accuracy_means, yerr=accuracy_stds, capsize=5, color='lightblue')
        ax1.set_ylabel('Accuracy')
        ax1.set_title('Few-Shot Learning Accuracy')
        ax1.set_ylim(0, 1)
        
        # F1 chart
        ax2.bar(domains, f1_means, yerr=f1_stds, capsize=5, color='lightgreen')
        ax2.set_ylabel('F1 Score')
        ax2.set_title('Few-Shot Learning F1 Score')
        ax2.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'few_shot_performance.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    print(f"‚úÖ Visualizations saved to: {output_dir}")

def main():
    """Main evaluation function"""
    print("üöÄ Starting Enhanced ABSA Evaluation with 2024-2025 Breakthrough Features")
    print("=" * 80)
    
    # Parse arguments
    args = parse_arguments()
    
    # Setup output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Setup logging
    logger = setup_logger('evaluation', os.path.join(args.output_dir, 'evaluation.log'))
    logger.info(f"Evaluation started with args: {vars(args)}")
    
    # Device setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üíª Using device: {device}")
    
    # Load model
    model, config, training_info = load_model_and_config(args.model, device)
    
    # Initialize tokenizer
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Initialize preprocessor
    preprocessor = ABSAPreprocessor(
        tokenizer=tokenizer,
        max_length=args.max_length,
        use_instruction_following=config.use_instruction_following
    )
    
    # Load datasets
    if args.dataset == 'all':
        # Multi-domain evaluation
        domains = ['laptop14', 'rest14', 'rest15', 'rest16']
        multi_domain_dataset = MultiDomainABSADataset(
            data_dir=args.data_dir,
            tokenizer=tokenizer,
            domains=domains,
            preprocessor=preprocessor,
            max_length=args.max_length,
            use_instruction_following=config.use_instruction_following
        )
        datasets = multi_domain_dataset
        
        # Create combined dataloader for standard evaluation
        all_samples = []
        for domain in domains:
            domain_dataset = multi_domain_dataset.datasets[domain][args.split]
            if domain_dataset is not None:
                all_samples.extend([domain_dataset[i] for i in range(len(domain_dataset))])
        
        # Create a temporary dataset from combined samples
        class CombinedDataset:
            def __init__(self, samples):
                self.samples = samples
            def __len__(self):
                return len(self.samples)
            def __getitem__(self, idx):
                return self.samples[idx]
        
        combined_dataset = CombinedDataset(all_samples)
        dataloader = DataLoader(
            combined_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            collate_fn=ABSADataset.collate_fn,
            num_workers=2
        )
        
    else:
        # Single domain evaluation
        dataset = ABSADataset(
            data_dir=args.data_dir,
            tokenizer=tokenizer,
            preprocessor=preprocessor,
            split=args.split,
            dataset_name=args.dataset,
            max_length=args.max_length
        )
        
        dataloader = DataLoader(
            dataset,
            batch_size=args.batch_size,
            shuffle=False,
            collate_fn=ABSADataset.collate_fn,
            num_workers=2
        )
        
        datasets = {args.dataset: {'test': dataset}}
    
    print(f"üìä Loaded dataset(s) for evaluation")
    
    # Initialize results dictionary
    results = {
        'model_info': {
            'model_path': args.model,
            'dataset': args.dataset,
            'split': args.split,
            'training_info': training_info
        },
        'evaluation_timestamp': datetime.now().isoformat(),
        'config': vars(config) if hasattr(config, '__dict__') else str(config)
    }
    
    # ============================================================================
    # STANDARD EVALUATION
    # ============================================================================
    
    print("\n" + "="*60)
    print("üìä STANDARD ABSA EVALUATION")
    print("="*60)
    
    standard_results = evaluate_standard_metrics(
        model, dataloader, device, config, 
        save_predictions=args.save_predictions
    )
    results['standard_evaluation'] = standard_results
    
    # ============================================================================
    # 2024-2025 BREAKTHROUGH EVALUATIONS
    # ============================================================================
    
    # Few-shot Learning Evaluation
    if args.eval_few_shot:
        print("\n" + "="*60)
        print("üéØ FEW-SHOT LEARNING EVALUATION")
        print("="*60)
        
        few_shot_results = evaluate_few_shot_performance(model, datasets, config, args)
        results['few_shot_evaluation'] = few_shot_results
    
    # Cross-domain Transfer Evaluation
    if args.eval_cross_domain and args.dataset == 'all':
        print("\n" + "="*60)
        print("üåê CROSS-DOMAIN TRANSFER EVALUATION")
        print("="*60)
        
        cross_domain_results = evaluate_cross_domain_transfer(model, datasets, config, args)
        results['cross_domain_evaluation'] = cross_domain_results
    
    # Implicit Detection Evaluation
    if args.eval_implicit:
        print("\n" + "="*60)
        print("üîç IMPLICIT DETECTION EVALUATION")
        print("="*60)
        
        implicit_results = evaluate_implicit_detection(model, dataloader, device, config)
        results['implicit_evaluation'] = implicit_results
    
    # Contrastive Learning Evaluation
    if args.eval_contrastive:
        print("\n" + "="*60)
        print("‚ö° CONTRASTIVE LEARNING EVALUATION")
        print("="*60)
        
        # This would require special evaluation of contrastive components
        # For now, we'll just note if the model has contrastive capabilities
        has_contrastive = hasattr(model, 'contrastive_learner') or config.use_contrastive_learning
        contrastive_results = {
            'has_contrastive_learning': has_contrastive,
            'contrastive_temperature': getattr(config, 'contrastive_temperature', 'N/A'),
            'status': 'available' if has_contrastive else 'not_available'
        }
        
        if has_contrastive:
            print("‚úÖ Model has contrastive learning capabilities")
            print(f"   Temperature: {contrastive_results['contrastive_temperature']}")
        else:
            print("‚ö† Model does not have contrastive learning capabilities")
        
        results['contrastive_evaluation'] = contrastive_results
    
    # ============================================================================
    # ADVANCED ANALYSIS
    # ============================================================================
    
    # Error Analysis
    if args.error_analysis and args.save_predictions:
        print("\n" + "="*60)
        print("üîç ERROR ANALYSIS")
        print("="*60)
        
        error_analysis = perform_error_analysis(standard_results, args.output_dir)
        results['error_analysis'] = error_analysis
    
    # ============================================================================
    # RESULTS SUMMARY AND SAVING
    # ============================================================================
    
    print("\n" + "="*60)
    print("üìã EVALUATION SUMMARY")
    print("="*60)
    
    # Print comprehensive summary
    standard_metrics = results['standard_evaluation']['metrics']
    print(f"üéØ Standard ABSA Performance:")
    print(f"   Overall F1: {standard_metrics.get('f1', 0):.4f}")
    print(f"   Precision: {standard_metrics.get('precision', 0):.4f}")
    print(f"   Recall: {standard_metrics.get('recall', 0):.4f}")
    print(f"   Aspect F1: {standard_metrics.get('aspect_f1', 0):.4f}")
    print(f"   Opinion F1: {standard_metrics.get('opinion_f1', 0):.4f}")
    print(f"   Sentiment Accuracy: {standard_metrics.get('sentiment_accuracy', 0):.4f}")
    
    if 'triplet_recovery_score' in standard_metrics:
        print(f"   Triplet Recovery Score: {standard_metrics['triplet_recovery_score']:.4f}")
    
    # Few-shot results summary
    if 'few_shot_evaluation' in results and results['few_shot_evaluation'].get('status') != 'not_available':
        print(f"\nüéØ Few-Shot Learning Performance:")
        few_shot_data = results['few_shot_evaluation']
        for domain, metrics in few_shot_data.items():
            if isinstance(metrics, dict) and 'accuracy_mean' in metrics:
                print(f"   {domain}: F1={metrics.get('f1_mean', 0):.3f}¬±{metrics.get('f1_std', 0):.3f}")
    
    # Cross-domain results summary
    if 'cross_domain_evaluation' in results and results['cross_domain_evaluation'].get('status') != 'not_available':
        print(f"\nüåê Cross-Domain Transfer Performance:")
        cross_domain_data = results['cross_domain_evaluation']
        for transfer_pair, metrics in cross_domain_data.items():
            if isinstance(metrics, dict):
                print(f"   {transfer_pair}: F1={metrics.get('macro_f1', 0):.3f}")
    
    # Implicit detection summary
    if 'implicit_evaluation' in results:
        implicit_metrics = results['implicit_evaluation']['implicit_metrics']
        print(f"\nüîç Implicit Detection Performance:")
        print(f"   Implicit F1: {implicit_metrics.get('f1', 0):.4f}")
        print(f"   Implicit Precision: {implicit_metrics.get('precision', 0):.4f}")
        print(f"   Implicit Recall: {implicit_metrics.get('recall', 0):.4f}")
    
    # Save results
    results_path = os.path.join(args.output_dir, f'evaluation_results_{args.dataset}_{args.split}.json')
    with open(results_path, 'w') as f:
        # Convert any numpy types to Python types for JSON serialization
        def convert_numpy(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            return obj
        
        json.dump(convert_numpy(results), f, indent=2)
    
    print(f"\n‚úÖ Results saved to: {results_path}")
    
    # Create visualizations
    if not args.error_analysis or not args.save_predictions:
        # Create basic visualizations even without full error analysis
        try:
            create_evaluation_visualizations(results, args.output_dir)
        except Exception as e:
            print(f"‚ö† Warning: Failed to create visualizations: {e}")
    
    # Save detailed predictions if requested
    if args.save_predictions and 'prediction_details' in standard_results:
        predictions_path = os.path.join(args.output_dir, f'predictions_{args.dataset}_{args.split}.json')
        with open(predictions_path, 'w') as f:
            json.dump(standard_results['prediction_details'], f, indent=2)
        print(f"‚úÖ Detailed predictions saved to: {predictions_path}")
    
    # ============================================================================
    # PUBLICATION-READY METRICS
    # ============================================================================
    
    print("\n" + "="*60)
    print("üìÑ PUBLICATION-READY METRICS")
    print("="*60)
    
    # Create publication summary
    pub_metrics = {
        'dataset': args.dataset,
        'split': args.split,
        'model_type': 'Enhanced ABSA with 2024-2025 Breakthroughs',
        'standard_metrics': {
            'f1': round(standard_metrics.get('f1', 0), 4),
            'precision': round(standard_metrics.get('precision', 0), 4),
            'recall': round(standard_metrics.get('recall', 0), 4),
            'aspect_f1': round(standard_metrics.get('aspect_f1', 0), 4),
            'opinion_f1': round(standard_metrics.get('opinion_f1', 0), 4),
            'sentiment_accuracy': round(standard_metrics.get('sentiment_accuracy', 0), 4),
        }
    }
    
    # Add breakthrough-specific metrics
    if 'triplet_recovery_score' in standard_metrics:
        pub_metrics['breakthrough_metrics'] = {
            'triplet_recovery_score': round(standard_metrics['triplet_recovery_score'], 4)
        }
    
    # Add few-shot metrics if available
    if 'few_shot_evaluation' in results and results['few_shot_evaluation'].get('status') != 'not_available':
        few_shot_avg = []
        for domain, metrics in results['few_shot_evaluation'].items():
            if isinstance(metrics, dict) and 'f1_mean' in metrics:
                few_shot_avg.append(metrics['f1_mean'])
        
        if few_shot_avg:
            pub_metrics['few_shot_f1_average'] = round(np.mean(few_shot_avg), 4)
    
    pub_metrics_path = os.path.join(args.output_dir, f'publication_metrics_{args.dataset}.json')
    with open(pub_metrics_path, 'w') as f:
        json.dump(pub_metrics, f, indent=2)
    
    print("üìä Publication-Ready Metrics:")
    for category, metrics in pub_metrics.items():
        if isinstance(metrics, dict):
            print(f"   {category}:")
            for metric, value in metrics.items():
                print(f"     {metric}: {value}")
        else:
            print(f"   {category}: {metrics}")
    
    print(f"‚úÖ Publication metrics saved to: {pub_metrics_path}")
    
    # ============================================================================
    # FINAL SUMMARY
    # ============================================================================
    
    print("\n" + "="*80)
    print("üéâ EVALUATION COMPLETED SUCCESSFULLY!")
    print("="*80)
    
    print(f"üìä Evaluated on: {args.dataset} ({args.split} split)")
    print(f"üéØ Overall F1 Score: {standard_metrics.get('f1', 0):.4f}")
    print(f"üìÅ Results saved to: {args.output_dir}")
    print(f"üìù Log file: {os.path.join(args.output_dir, 'evaluation.log')}")
    
    # Provide recommendations based on results
    print(f"\nüí° Recommendations for Publication:")
    
    f1_score = standard_metrics.get('f1', 0)
    if f1_score >= 0.85:
        print("   ‚úÖ Excellent performance - ready for top-tier venues")
    elif f1_score >= 0.75:
        print("   ‚úÖ Good performance - suitable for publication with additional analysis")
    elif f1_score >= 0.65:
        print("   ‚ö† Moderate performance - consider additional improvements")
    else:
        print("   ‚ùå Performance needs significant improvement before publication")
    
    # Check for breakthrough features
    breakthrough_count = 0
    if results.get('contrastive_evaluation', {}).get('has_contrastive_learning', False):
        breakthrough_count += 1
    if results.get('few_shot_evaluation', {}).get('status') != 'not_available':
        breakthrough_count += 1
    if results.get('cross_domain_evaluation', {}).get('status') != 'not_available':
        breakthrough_count += 1
    if results.get('implicit_evaluation', {}):
        breakthrough_count += 1
    
    print(f"   üöÄ Implemented {breakthrough_count}/4 breakthrough features from 2024-2025")
    
    if breakthrough_count >= 3:
        print("   ‚úÖ Strong alignment with 2024-2025 ABSA trends")
        print("   üìö Recommended venues: EMNLP, ACL, NAACL")
    elif breakthrough_count >= 2:
        print("   ‚ö† Good alignment - consider implementing remaining features")
        print("   üìö Recommended venues: EMNLP Findings, NAACL Findings")
    else:
        print("   ‚ùå Limited alignment with current trends - implement more breakthrough features")
    
    logger.info("Evaluation completed successfully")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n‚ùå Evaluation interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Evaluation failed with error: {e}")
        traceback.print_exc()