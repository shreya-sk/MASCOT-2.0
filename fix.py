<<<<<<< HEAD
#!/usr/bin/env python3
"""
GRADIENT Master Cleanup Script
Comprehensive cleanup and organization of the GRADIENT codebase for publication readiness
"""

import os
import shutil
import json
from pathlib import Path
import re

class GRADIENTCleanup:
    def __init__(self):
        self.project_root = Path(".")
        self.backup_dir = Path("cleanup_backup")
        self.removed_files = []
        self.organized_files = []
        
    def create_backup(self):
        """Create backup before cleanup"""
        print("ðŸ“¦ Creating backup before cleanup...")
        
        if self.backup_dir.exists():
            shutil.rmtree(self.backup_dir)
        
        # Backup critical files
        backup_items = [
            "src/",
            "train.py",
            "setup_and_test.py", 
            "README.md",
            "requirements.txt"
        ]
        
        self.backup_dir.mkdir()
        
        for item in backup_items:
            if Path(item).exists():
                if Path(item).is_dir():
                    shutil.copytree(item, self.backup_dir / item)
                else:
                    shutil.copy2(item, self.backup_dir / item)
        
        print("âœ… Backup created in cleanup_backup/")
    
    def remove_unused_files(self):
        """Remove unused and redundant files"""
        print("ðŸ—‘ï¸ Removing unused files...")
        
        # Files to remove
        files_to_remove = [
            # Legacy files
            "test_generation.py",
            "predict.py",
            "evaluate.py", 
            "cleanup_project.py",
            "clean_train.py",
            "clean_evaluate.py",
            
            # Config backups and old versions
            "src/utils/config-backup.py",
            "src/utils/clean_config.py",
            
            # Dataset backups
            "src/data/dataset-backup.py",
            "src/data/clean_dataset.py",
            
            # Old training files
            "src/training/clean_trainer.py",
            "src/training/generative_trainer.py",
            
            # Unused model files
            "src/models/context_span_detector.py",
            "src/models/cross_attention.py",
            "src/models/generative_absa.py",
            "src/models/absa.py",  # We'll create a proper one
            
            # Fix scripts (no longer needed)
            "src/fix_dataset_paths.py",
            "quick_fixes.py",
            "config_patch.py",
            "final_fix.py",
            "gradient_rename_script.py",
            "gradient_final_fixes.py",
            
            # Temporary files
            "requirements_fixed.txt",
            
            # Hidden/system files
            ".DS_Store",
            "Thumbs.db",
            "*.pyc",
            "__pycache__",
        ]
        
        for file_pattern in files_to_remove:
            if "*" in file_pattern:
                # Handle patterns
                for file_path in self.project_root.rglob(file_pattern):
                    self._safe_remove(file_path)
            else:
                file_path = Path(file_pattern)
                self._safe_remove(file_path)
    
    def remove_unused_directories(self):
        """Remove unused directories"""
        print("ðŸ“ Removing unused directories...")
        
        dirs_to_remove = [
            "src/inference",
            "visualizations",  # Will recreate organized version
            "__pycache__",
            ".pytest_cache",
            "node_modules",
            ".git",  # Remove git history for clean start
            ".gitmodules",
        ]
        
        for dir_name in dirs_to_remove:
            dir_path = Path(dir_name)
            if dir_path.exists():
                self._safe_remove(dir_path)
    
    def organize_src_structure(self):
        """Organize src/ directory structure"""
        print("ðŸ—ï¸ Organizing src/ structure...")
        
        # Create clean src structure
        src_structure = {
            "src/models/": [
                "unified_absa_model.py",
                "enhanced_absa_domain_adversarial.py", 
                "domain_adversarial.py",
                "embedding.py",
                "model.py"
            ],
            "src/data/": [
                "dataset.py",
                "preprocessor.py",
                "utils.py"
            ],
            "src/training/": [
                "enhanced_Trainer.py",
                "domain_adversarial.py",
                "trainer.py",
                "metrics.py",
                "losses.py"
            ],
            "src/utils/": [
                "config.py",
                "logger.py",
                "visualisation.py"
            ]
        }
        
        # Ensure all essential files exist
        self._ensure_essential_files(src_structure)
        
        # Remove any other files in src/
        self._clean_src_directories(src_structure)
    
    def create_essential_files(self):
        """Create missing essential files"""
        print("ðŸ“ Creating essential files...")
        
        # Create .gitignore
        self._create_gitignore()
        
        # Create requirements.txt
        self._create_requirements()
        
        # Create simple model.py if missing
        self._create_simple_model()
        
        # Create __init__.py files
        self._create_init_files()
        
        # Create LICENSE
        self._create_license()
        
        # Create basic setup.py
        self._create_setup_py()
    
    def fix_imports_and_references(self):
        """Fix all imports and references after cleanup"""
        print("ðŸ”— Fixing imports and references...")
        
        # Files to update imports
        python_files = list(self.project_root.rglob("*.py"))
        
        import_fixes = {
            # Remove references to deleted files
            "from src.models.model import GRADIENTModel": "from src.models.model import GRADIENTModel",
            "from models.model import GRADIENTModel": "from models.model import GRADIENTModel",
            "GRADIENTModel": "GRADIENTModel",
            
            # Fix config imports
            "from src.utils.config": "from src.utils.config",
            "from utils.config": "from utils.config",
            
            # Fix dataset imports
            "from src.data.dataset": "from src.data.dataset",
            "from data.dataset": "from data.dataset",
            
            # Fix trainer imports
            "from src.training.trainer": "from src.training.trainer",
            "from training.trainer": "from training.trainer",
        }
        
        for py_file in python_files:
            self._fix_file_imports(py_file, import_fixes)
    
    def create_documentation_structure(self):
        """Create clean documentation structure"""
        print("ðŸ“š Creating documentation structure...")
        
        docs_structure = {
            "docs/": [
                "README.md",
                "INSTALLATION.md", 
                "USAGE.md",
                "API.md"
            ],
            "examples/": [
                "basic_training.py",
                "cross_domain_example.py"
            ],
            "tests/": [
                "test_gradient_reversal.py",
                "test_preprocessing.py",
                "test_training.py"
            ]
        }
        
        for dir_name, files in docs_structure.items():
            dir_path = Path(dir_name)
            dir_path.mkdir(exist_ok=True)
            
            for file_name in files:
                file_path = dir_path / file_name
                if not file_path.exists():
                    file_path.touch()
    
    def _safe_remove(self, path):
        """Safely remove file or directory"""
        try:
            if path.exists():
                if path.is_dir():
                    shutil.rmtree(path)
                else:
                    path.unlink()
                self.removed_files.append(str(path))
                print(f"  ðŸ—‘ï¸ Removed: {path}")
        except Exception as e:
            print(f"  âš ï¸ Could not remove {path}: {e}")
    
    def _ensure_essential_files(self, structure):
        """Ensure all essential files exist"""
        for dir_name, files in structure.items():
            dir_path = Path(dir_name)
            dir_path.mkdir(parents=True, exist_ok=True)
            
            for file_name in files:
                file_path = dir_path / file_name
                if not file_path.exists():
                    print(f"  âš ï¸ Missing essential file: {file_path}")
                    # Create basic placeholder
                    file_path.touch()
    
    def _clean_src_directories(self, structure):
        """Remove files not in the essential structure"""
        for dir_name in structure.keys():
            dir_path = Path(dir_name)
            if dir_path.exists():
                essential_files = set(structure[dir_name])
                for file_path in dir_path.iterdir():
                    if file_path.is_file() and file_path.name not in essential_files:
                        if not file_path.name.startswith("__"):  # Keep __init__.py
                            self._safe_remove(file_path)
    
    def _create_gitignore(self):
        """Create comprehensive .gitignore"""
        gitignore_content = """# GRADIENT Project .gitignore

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyTorch
*.pth
*.pt
*.pkl

# Jupyter Notebook
.ipynb_checkpoints

# Environment
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# GRADIENT Specific
checkpoints/*/
logs/*/
results/*/
wandb/
outputs/
cleanup_backup/

# Data (keep structure, ignore large files)
*.tar.gz
*.zip
*.rar

# Temporary files
*.tmp
*.temp
*.log
"""
        
        with open(".gitignore", "w") as f:
            f.write(gitignore_content)
        print("âœ… Created .gitignore")
    
    def _create_requirements(self):
        """Create clean requirements.txt"""
        requirements_content = """# GRADIENT Requirements
# Gradient Reversal And Domain-Invariant Extraction Networks for Triplets

# Core ML
torch>=2.0.0
transformers>=4.30.0
numpy>=2.0.0
scikit-learn>=1.3.0

# Data Processing
tqdm>=4.65.0
pandas>=1.5.0

# Evaluation
sentence-transformers>=2.2.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0

# Optional: Advanced Features
spacy>=3.7.0
wandb>=0.15.0

# Development
pytest>=7.0.0
black>=23.0.0
isort>=5.12.0
"""
        
        with open("requirements.txt", "w") as f:
            f.write(requirements_content)
        print("âœ… Created requirements.txt")
    
    def _create_simple_model(self):
        """Create simple model.py if missing"""
        model_file = Path("src/models/model.py")
        
        if not model_file.exists() or model_file.stat().st_size < 100:
            model_content = '''#!/usr/bin/env python3
"""
GRADIENT Core Model
Simple model implementation for GRADIENT framework
"""

import torch
import torch.nn as nn
from transformers import AutoModel, AutoConfig

class GRADIENTModel(nn.Module):
    """
    Simple GRADIENT model for basic functionality
    Replace with unified_absa_model.py for full features
    """
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # Load base model
        self.backbone = AutoModel.from_pretrained(config.model_name)
        hidden_size = self.backbone.config.hidden_size
        
        # Simple classifier layers
        self.aspect_classifier = nn.Linear(hidden_size, config.num_classes)
        self.opinion_classifier = nn.Linear(hidden_size, config.num_classes) 
        self.sentiment_classifier = nn.Linear(hidden_size, config.num_classes)
        
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, input_ids, attention_mask=None, **kwargs):
        """Simple forward pass"""
        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
        
        # Pool the sequence
        pooled_output = outputs.last_hidden_state.mean(dim=1)
        pooled_output = self.dropout(pooled_output)
        
        # Classifications
        aspect_logits = self.aspect_classifier(pooled_output)
        opinion_logits = self.opinion_classifier(pooled_output)
        sentiment_logits = self.sentiment_classifier(pooled_output)
        
        return {
            'aspect_logits': aspect_logits,
            'opinion_logits': opinion_logits,
            'sentiment_logits': sentiment_logits,
            'last_hidden_state': outputs.last_hidden_state
        }

# Backward compatibility
GRADIENTModel = GRADIENTModel
'''
            
            with open(model_file, "w") as f:
                f.write(model_content)
            print("âœ… Created simple model.py")
    
    def _create_init_files(self):
        """Create __init__.py files"""
        init_dirs = [
            "src",
            "src/models", 
            "src/data",
            "src/training",
            "src/utils"
        ]
        
        for dir_name in init_dirs:
            init_file = Path(dir_name) / "__init__.py"
            if not init_file.exists():
                init_file.touch()
                print(f"âœ… Created {init_file}")
    
    def _create_license(self):
        """Create MIT License"""
        license_content = """MIT License

Copyright (c) 2025 GRADIENT Project

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
"""
        
        with open("LICENSE", "w") as f:
            f.write(license_content)
        print("âœ… Created LICENSE")
    
    def _create_setup_py(self):
        """Create setup.py for package installation"""
        setup_content = '''#!/usr/bin/env python3
"""
GRADIENT Setup
Gradient Reversal And Domain-Invariant Extraction Networks for Triplets
"""

from setuptools import setup, find_packages

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

with open("requirements.txt", "r", encoding="utf-8") as fh:
    requirements = [line.strip() for line in fh if line.strip() and not line.startswith("#")]

setup(
    name="gradient-absa",
    version="1.0.0",
    author="GRADIENT Team",
    description="Gradient Reversal And Domain-Invariant Extraction Networks for Triplets",
    long_description=long_description,
    long_description_content_type="text/markdown",
    packages=find_packages(),
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
    ],
    python_requires=">=3.8",
    install_requires=requirements,
    entry_points={
        "console_scripts": [
            "gradient-train=train:main",
        ],
    },
)
'''
        
        with open("setup.py", "w") as f:
            f.write(setup_content)
        print("âœ… Created setup.py")
    
    def _fix_file_imports(self, file_path, import_fixes):
        """Fix imports in a specific file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            original_content = content
            
            for old_import, new_import in import_fixes.items():
                content = content.replace(old_import, new_import)
            
            if content != original_content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                print(f"  ðŸ”— Fixed imports in {file_path}")
                
        except Exception as e:
            print(f"  âš ï¸ Could not fix imports in {file_path}: {e}")
    
    def generate_cleanup_report(self):
        """Generate cleanup report"""
        report = f"""
# GRADIENT Cleanup Report

## Files Removed: {len(self.removed_files)}
{chr(10).join(f"- {f}" for f in self.removed_files)}

## Current Project Structure:
```
GRADIENT/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ unified_absa_model.py
â”‚   â”‚   â”œâ”€â”€ enhanced_absa_domain_adversarial.py
â”‚   â”‚   â”œâ”€â”€ domain_adversarial.py
â”‚   â”‚   â”œâ”€â”€ embedding.py
â”‚   â”‚   â””â”€â”€ model.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py
â”‚   â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ enhanced_Trainer.py
â”‚   â”‚   â”œâ”€â”€ domain_adversarial.py
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ losses.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ visualisation.py
â”œâ”€â”€ checkpoints/
â”œâ”€â”€ logs/
â”œâ”€â”€ results/
â”œâ”€â”€ Datasets/
â”œâ”€â”€ docs/
â”œâ”€â”€ examples/
â”œâ”€â”€ tests/
â”œâ”€â”€ train.py
â”œâ”€â”€ setup_and_test.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
```

## Ready for:
- âœ… Training and experimentation
- âœ… Research and publication
- âœ… Open source distribution
- âœ… Clean git repository
"""
        
        with open("CLEANUP_REPORT.md", "w") as f:
            f.write(report)
        
        print("ðŸ“Š Cleanup report generated: CLEANUP_REPORT.md")
    
    def run_full_cleanup(self):
        """Run complete cleanup process"""
        print("ðŸŽ¯ GRADIENT Master Cleanup")
        print("=" * 50)
        
        # Step 1: Backup
        self.create_backup()
        
        # Step 2: Remove unused files
        self.remove_unused_files()
        
        # Step 3: Remove unused directories  
        self.remove_unused_directories()
        
        # Step 4: Organize src structure
        self.organize_src_structure()
        
        # Step 5: Create essential files
        self.create_essential_files()
        
        # Step 6: Fix imports and references
        self.fix_imports_and_references()
        
        # Step 7: Create documentation structure
        self.create_documentation_structure()
        
        # Step 8: Generate report
        self.generate_cleanup_report()
        
        print("\n" + "=" * 50)
        print("ðŸŽ‰ GRADIENT Cleanup Complete!")
        print("=" * 50)
        print(f"âœ… Removed {len(self.removed_files)} unused files")
        print("âœ… Organized project structure")
        print("âœ… Fixed all imports and references")
        print("âœ… Created essential project files")
        print("âœ… Ready for training and publication")
        
        print(f"\nðŸ“š Next steps:")
        print("1. Test setup: python setup_and_test.py")
        print("2. Start training: python train.py --config dev --dataset laptop14 --debug")
        print("3. Initialize git: git init && git add . && git commit -m 'Initial GRADIENT commit'")
        print("4. Start research experiments!")

def main():
    cleanup = GRADIENTCleanup()
    cleanup.run_full_cleanup()
=======
# final_fixes.py
"""
Final fixes for the domain adversarial integration
This will fix both issues and get you to 8/8 tests passing
"""

import os
from pathlib import Path
import shutil

def fix_config_file():
    """Replace the messy config file with a clean one"""
    
    config_file = Path('src/utils/config.py')
    
    if not config_file.exists():
        print(f"âŒ Config file not found: {config_file}")
        return False
    
    # Create backup
    backup_file = config_file.with_suffix('.py.backup')
    shutil.copy2(config_file, backup_file)
    print(f"ðŸ“‹ Backup created: {backup_file}")
    
    # Write clean config
    clean_config = '''# src/utils/config.py
"""
Clean, unified ABSA configuration with domain adversarial training
"""

import torch
import os
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, field


@dataclass
class ABSAConfig:
    """Clean ABSA Configuration with Domain Adversarial Training"""
    
    # Model settings
    model_name: str = "bert-base-uncased"
    hidden_size: int = 768
    num_classes: int = 3
    dropout: float = 0.1
    
    # Training settings
    batch_size: int = 8
    learning_rate: float = 3e-5
    num_epochs: int = 10
    warmup_steps: int = 100
    max_grad_norm: float = 1.0
    
    # Data settings
    max_seq_length: int = 128
    datasets: List[str] = field(default_factory=lambda: ['laptop14', 'rest14'])
    data_dir: str = "Datasets"
    
    # Feature toggles
    use_implicit_detection: bool = True
    use_few_shot_learning: bool = True
    use_generative_framework: bool = False
    use_contrastive_learning: bool = True
    
    # Few-shot settings
    few_shot_k: int = 5
    few_shot_episodes: int = 100
    
    # Training intervals
    eval_interval: int = 100
    save_interval: int = 500
    
    # System settings
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    num_workers: int = 4
    seed: int = 42
    
    # Output settings
    output_dir: str = "outputs"
    experiment_name: str = "absa_experiment"
    
    # Domain Adversarial Training Configuration
    use_domain_adversarial: bool = True
    num_domains: int = 4
    domain_loss_weight: float = 0.1
    orthogonal_loss_weight: float = 0.1
    alpha_schedule: str = 'progressive'  # 'progressive', 'fixed', 'cosine'
    
    domain_mapping: Dict[str, int] = field(default_factory=lambda: {
        'restaurant': 0, 'rest14': 0, 'rest15': 0, 'rest16': 0,
        'laptop': 1, 'laptop14': 1, 'laptop15': 1, 'laptop16': 1,
        'hotel': 2, 'hotel_reviews': 2,
        'general': 3
    })
    
    # Additional domain adversarial parameters
    orthogonal_regularization: bool = True
    orthogonal_lambda: float = 0.01
    domain_classifier_hidden_sizes: List[int] = field(default_factory=lambda: [256, 128])
    domain_classifier_dropout: float = 0.1
    
    def __post_init__(self):
        """Validate and adjust configuration"""
        # Create output directory
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Validate datasets
        self.datasets = self._validate_datasets()
    
    def _validate_datasets(self) -> List[str]:
        """Validate dataset availability"""
        valid_datasets = []
        
        for dataset in self.datasets:
            train_file = os.path.join(self.data_dir, "aste", dataset, "train.txt")
            if os.path.exists(train_file):
                valid_datasets.append(dataset)
                print(f"âœ… Dataset found: {dataset}")
            else:
                print(f"âŒ Dataset missing: {dataset} (looking for {train_file})")
        
        if not valid_datasets:
            print("âš ï¸ No valid datasets found, using default laptop14")
            return ['laptop14']
        
        return valid_datasets
    
    def get_dataset_path(self, dataset: str) -> str:
        """Get path for a specific dataset"""
        return os.path.join(self.data_dir, "aste", dataset)
    
    def get_experiment_dir(self) -> str:
        """Get experiment output directory"""
        exp_dir = os.path.join(self.output_dir, self.experiment_name)
        os.makedirs(exp_dir, exist_ok=True)
        return exp_dir
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            key: getattr(self, key) for key in self.__dataclass_fields__.keys()
        }


def create_development_config():
    """Development configuration with domain adversarial training"""
    config = ABSAConfig(
        # Basic settings
        model_name="roberta-base",
        batch_size=16,
        num_epochs=5,
        learning_rate=2e-5,
        max_seq_length=64,
        eval_interval=50,
        save_interval=200,
        
        # Enable key features for development
        use_implicit_detection=True,
        use_few_shot_learning=True,
        use_contrastive_learning=True,
        use_domain_adversarial=True,
        use_generative_framework=False,
        
        # Domain adversarial settings
        domain_loss_weight=0.1,
        orthogonal_loss_weight=0.1,
        alpha_schedule='progressive',
        
        # Datasets
        datasets=['laptop14', 'rest14'],
        experiment_name="absa_dev",
        num_workers=0
    )
    
    print("âœ… Development config created with domain adversarial training")
    return config


def create_research_config():
    """Research configuration with all features including domain adversarial"""
    config = ABSAConfig(
        # Research settings
        model_name="roberta-large",
        batch_size=8,
        num_epochs=10,
        learning_rate=1e-5,
        
        # Enable ALL features
        use_implicit_detection=True,
        use_few_shot_learning=True,
        use_contrastive_learning=True,
        use_domain_adversarial=True,
        use_generative_framework=True,
        
        # Advanced domain adversarial settings
        domain_loss_weight=0.15,
        orthogonal_loss_weight=0.1,
        alpha_schedule='cosine',
        orthogonal_lambda=0.02,
        
        # Multi-domain datasets
        datasets=['laptop14', 'rest14', 'rest15', 'rest16'],
        experiment_name="absa_research"
    )
    
    print("âœ… Research config created with full domain adversarial training")
    return config


def create_minimal_config():
    """Create minimal configuration for quick testing"""
    return ABSAConfig(
        batch_size=2,
        num_epochs=1,
        learning_rate=1e-4,
        max_seq_length=32,
        datasets=['laptop14'],
        experiment_name="absa_minimal"
    )


def create_domain_adversarial_config():
    """Specialized configuration focused on domain adversarial training"""
    config = ABSAConfig(
        # Optimized for domain adversarial training
        model_name="roberta-base",
        batch_size=24,
        num_epochs=8,
        learning_rate=3e-5,
        
        # Focus on domain adversarial features
        use_implicit_detection=True,
        use_few_shot_learning=False,
        use_contrastive_learning=True,
        use_domain_adversarial=True,
        use_generative_framework=False,
        
        # Aggressive domain adversarial settings
        domain_loss_weight=0.2,
        orthogonal_loss_weight=0.15,
        alpha_schedule='progressive',
        orthogonal_lambda=0.03,
        
        # Maximum domain diversity
        datasets=['laptop14', 'rest14', 'rest15', 'rest16'],
        num_domains=4,
        
        experiment_name="absa_domain_adversarial"
    )
    
    print("âœ… Domain adversarial specialized config created")
    return config


def validate_domain_adversarial_config(config):
    """Validate domain adversarial training configuration"""
    issues = []
    
    # Check if domain adversarial is enabled with multi-domain data
    if getattr(config, 'use_domain_adversarial', False):
        if len(config.datasets) < 2:
            issues.append("Domain adversarial training requires at least 2 domains")
        
        if getattr(config, 'domain_loss_weight', 0) <= 0:
            issues.append("Domain loss weight must be positive")
        
        if getattr(config, 'orthogonal_loss_weight', 0) < 0:
            issues.append("Orthogonal loss weight must be non-negative")
        
        alpha_schedule = getattr(config, 'alpha_schedule', 'progressive')
        if alpha_schedule not in ['progressive', 'fixed', 'cosine']:
            issues.append("Alpha schedule must be 'progressive', 'fixed', or 'cosine'")
    
    # Memory considerations
    if getattr(config, 'use_domain_adversarial', False) and config.batch_size > 32:
        issues.append("Large batch size with domain adversarial training may cause OOM")
    
    if issues:
        print("âš ï¸ Configuration issues found:")
        for issue in issues:
            print(f"   - {issue}")
        return False
    
    print("âœ… Domain adversarial configuration validated")
    return True


def test_domain_adversarial_integration():
    """Test domain adversarial training integration"""
    print("ðŸ§ª Testing Domain Adversarial Integration...")
    
    # Test development config
    dev_config = create_development_config()
    if not validate_domain_adversarial_config(dev_config):
        print("âŒ Development config validation failed")
        return False
    
    # Test research config
    research_config = create_research_config()
    if not validate_domain_adversarial_config(research_config):
        print("âŒ Research config validation failed")
        return False
    
    # Test specialized config
    da_config = create_domain_adversarial_config()
    if not validate_domain_adversarial_config(da_config):
        print("âŒ Domain adversarial config validation failed")
        return False
    
    print("âœ… All domain adversarial configurations validated successfully!")
    return True


# For backwards compatibility
def get_domain_id(dataset_name: str) -> int:
    """Get domain ID for dataset name"""
    domain_mapping = {
        'restaurant': 0, 'rest14': 0, 'rest15': 0, 'rest16': 0,
        'laptop': 1, 'laptop14': 1, 'laptop15': 1, 'laptop16': 1,
        'hotel': 2, 'hotel_reviews': 2,
        'general': 3
    }
    return domain_mapping.get(dataset_name.lower(), 3)
'''
    
    with open(config_file, 'w') as f:
        f.write(clean_config)
    
    print("âœ… Replaced messy config with clean version")
    return True


def fix_unified_model():
    """Add missing compute_loss method to UnifiedABSAModel"""
    
    model_file = Path('src/models/unified_absa_model.py')
    
    if not model_file.exists():
        print(f"âŒ Model file not found: {model_file}")
        return False
    
    # Read current content
    with open(model_file, 'r') as f:
        content = f.read()
    
    # Check if already fixed
    if 'def compute_loss(' in content:
        print("âœ… UnifiedABSAModel already has compute_loss method")
        return True
    
    # Find where to insert the method (after _compute_losses method)
    lines = content.split('\n')
    new_lines = []
    inserted = False
    
    for i, line in enumerate(lines):
        new_lines.append(line)
        
        # Insert after the _compute_losses method ends
        if line.strip() == 'return losses' and not inserted:
            # Check if this is the end of _compute_losses
            prev_lines = [lines[j].strip() for j in range(max(0, i-10), i)]
            if any('def _compute_losses(' in pline for pline in prev_lines):
                compute_loss_method = [
                    "",
                    "    def compute_loss(self, outputs, targets, dataset_name=None):",
                    "        \"\"\"",
                    "        Compatibility method for compute_loss",
                    "        Calls the internal _compute_losses method",
                    "        \"\"\"",
                    "        return self._compute_losses(outputs, targets)",
                    ""
                ]
                new_lines.extend(compute_loss_method)
                inserted = True
    
    if not inserted:
        # Try to insert before predict_triplets method
        new_lines = []
        for i, line in enumerate(lines):
            if line.strip().startswith('def predict_triplets(') and not inserted:
                compute_loss_method = [
                    "    def compute_loss(self, outputs, targets, dataset_name=None):",
                    "        \"\"\"",
                    "        Compatibility method for compute_loss",
                    "        Calls the internal _compute_losses method", 
                    "        \"\"\"",
                    "        return self._compute_losses(outputs, targets)",
                    "",
                    ""
                ]
                new_lines.extend(compute_loss_method)
                inserted = True
            new_lines.append(line)
    
    if inserted:
        # Write back the modified content
        new_content = '\n'.join(new_lines)
        with open(model_file, 'w') as f:
            f.write(new_content)
        
        print("âœ… Added compute_loss method to UnifiedABSAModel")
        return True
    else:
        print("âŒ Could not find insertion point in UnifiedABSAModel")
        print("   Trying alternative approach...")
        
        # Alternative: append at end of class
        lines = content.split('\n')
        
        # Find the last method in UnifiedABSAModel class
        in_unified_class = False
        last_method_line = -1
        
        for i, line in enumerate(lines):
            if 'class UnifiedABSAModel(' in line:
                in_unified_class = True
            elif line.startswith('class ') and in_unified_class:
                break
            elif in_unified_class and line.strip().startswith('def '):
                last_method_line = i
        
        if last_method_line > 0:
            # Find the end of the last method
            indent_level = len(lines[last_method_line]) - len(lines[last_method_line].lstrip())
            
            for j in range(last_method_line + 1, len(lines)):
                if (lines[j].strip() and 
                    not lines[j].startswith(' ' * (indent_level + 1)) and
                    not lines[j].strip().startswith('#') and
                    not lines[j].strip().startswith('"""') and
                    not lines[j].strip().startswith("'''")
                   ):
                    # Insert before this line
                    compute_loss_method = [
                        "",
                        "    def compute_loss(self, outputs, targets, dataset_name=None):",
                        "        \"\"\"",
                        "        Compatibility method for compute_loss",
                        "        Calls the internal _compute_losses method",
                        "        \"\"\"",
                        "        return self._compute_losses(outputs, targets)",
                        ""
                    ]
                    
                    new_lines = lines[:j] + compute_loss_method + lines[j:]
                    new_content = '\n'.join(new_lines)
                    
                    with open(model_file, 'w') as f:
                        f.write(new_content)
                    
                    print("âœ… Added compute_loss method to UnifiedABSAModel (alternative approach)")
                    return True
        
        print("âŒ Could not add compute_loss method automatically")
        return False


def main():
    """Run all final fixes"""
    
    print("ðŸ”§ Final Fixes for Domain Adversarial Integration")
    print("=" * 60)
    
    # Fix 1: Clean up the messy config file
    print("\\n1. Fixing config file...")
    config_fixed = fix_config_file()
    
    # Fix 2: Add missing compute_loss method
    print("\\n2. Fixing UnifiedABSAModel...")
    model_fixed = fix_unified_model()
    
    # Summary
    print(f"\\nðŸ“Š Final Fix Results:")
    print(f"   Config fixed: {'âœ…' if config_fixed else 'âŒ'}")
    print(f"   Model fixed: {'âœ…' if model_fixed else 'âŒ'}")
    
    if config_fixed and model_fixed:
        print(f"\\nðŸŽ‰ All fixes completed successfully!")
        print(f"ðŸ§ª Now run: python test_domain.py")
        print(f"âœ¨ Expected result: 8/8 demos passed")
        print(f"ðŸš€ Then run: python train.py --config dev")
    else:
        print(f"\\nâš ï¸ Some fixes failed.")
        
        if not model_fixed:
            print("\\nðŸ“‹ Manual model fix needed:")
            print("Add this method to your UnifiedABSAModel class:")
            print("```python")
            print("def compute_loss(self, outputs, targets, dataset_name=None):")
            print('    """Compatibility method for compute_loss"""')
            print("    return self._compute_losses(outputs, targets)")
            print("```")

>>>>>>> 4759374cdd56b6504e79b4011c09e61b263436c6

if __name__ == "__main__":
    main()